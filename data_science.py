# -*- coding: utf-8 -*-
"""data science.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12A-3m8W3sx7dklCMgHgWU8Or4O6JRqqH

#DEVELOP A FULL DATA SCIENCE PROJECT,FROM DATA COLLECTION AND PREPROCESSING TO MODEL DEPLOYMENT USING FLASK OR FASTAPI.
"""

!pip install imbalanced-learn

# Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
!pip install imbalanced-learn

# Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
!pip install imblearn
from imblearn.over_sampling import SMOTE # Corrected import
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import joblib


# Load dataset
data = pd.read_csv('sensor-data.csv')

# Data exploration
print("Dataset Shape:", data.shape)
print(data.info())
print(data.describe())
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
import joblib


# Load dataset
data = pd.read_csv('sensor-data.csv')

# Data exploration
print("Dataset Shape:", data.shape)
print(data.info())
print(data.describe())

# Preprocessing
# Check the actual column names in your DataFrame
print(data.columns)

# Replace 'your_actual_target_column_name' with the actual target column name from the output above
target_column_name = 'power'  # Replace 'power' with the actual name if it's different

X = data.drop(columns=[target_column_name])
y = data[target_column_name]

# Handle missing values
# Create a SimpleImputer instance with 'most_frequent' strategy for non-numeric columns
imputer_categorical = SimpleImputer(strategy='most_frequent')

# Create a SimpleImputer instance with 'mean' strategy for numeric columns
imputer_numerical = SimpleImputer(strategy='mean')

# Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object']).columns  # Select columns with object dtype (likely string or datetime)
numerical_cols = X.select_dtypes(include=['number']).columns  # Select columns with numeric dtype

# Apply imputation to respective columns
X[categorical_cols] = imputer_categorical.fit_transform(X[categorical_cols])
X[numerical_cols] = imputer_numerical.fit_transform(X[numerical_cols])

# Convert 'time' column to numerical representation (e.g., Unix timestamp)
X['time'] = pd.to_datetime(X['time']).astype(int) / 10**9  # Convert to Unix timestamp
# Standardize the data
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Example using weighted regression with RandomForestRegressor:
from sklearn.ensemble import RandomForestRegressor
from sklearn.utils.class_weight import compute_sample_weight

# Calculate sample weights
sample_weights = compute_sample_weight(class_weight='balanced', y=y)

# Initialize and train the regressor
regressor = RandomForestRegressor(random_state=42)
regressor.fit(X, y, sample_weight=sample_weights)

# 3. Model Training
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split # Import train_test_split

# Handle class imbalance
!pip install imbalanced-learn
!pip install imblearn
def handle_imbalance(X, y):
    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, y)
    return X_resampled, y_resampled

# 4. Save the Best Model
import joblib
from sklearn.ensemble import RandomForestRegressor # Import RandomForestRegressor

# Assuming RandomForestRegressor is the intended model
rf_model = RandomForestRegressor(random_state=42) # Initialize the model

# Fit the model (assuming X and y are preprocessed and ready)
# Replace X and y with the actual data used for training
rf_model.fit(X, y)

# Save the Random Forest model (assuming it's the best based on metrics)
joblib.dump(rf_model, 'best_model.pkl')

!pip install flask flask-ngrok

from flask import Flask, request, jsonify
import joblib
import pandas as pd

# Load your model
model = joblib.load('best_model.pkl')  # Ensure this file is in the Colab environment

# Initialize Flask app
app = Flask(__name__)

@app.route('/')
def home():
    return "Semiconductor Yield Prediction API is running."

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # Parse JSON input
        data = request.get_json()

        # Convert JSON to DataFrame
        input_data = pd.DataFrame([data])

        # Preprocess input (if necessary)
        # Preprocessing steps like scaling or imputing should be added here

        # Predict
        prediction = model.predict(input_data)[0]
        return jsonify({'prediction': int(prediction)})
    except Exception as e:
        return jsonify({'error': str(e)})

# Run the Flask app
if __name__ == '__main__':
    app.run()

from flask_ngrok import run_with_ngrok

# Start Flask app
run_with_ngrok(app)  # This automatically creates a public URL for your app
app.run()